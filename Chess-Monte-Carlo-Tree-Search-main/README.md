# Instructions

To play against this chess AI, simply run chess_game.py

To watch the AI play against Stockfish, replace the path in the versus_stockfish.py function with the path to Stockfish on your machine and run

# Video Presentation:
[YouTube Link](https://youtu.be/sJXaGTqkeD8)

# Here is the associated write-up:

For my final project for Game AI, I chose to implement Monte Carlo Tree Search (MCTS) for chess. The implementation was done in Python, utilizing the python-chess library to handle the intricacies of chess gameplay. Going into the project, my expected focus areas were managing the breadth vs. depth of the search, managing resources as the search tree was built, and adding in domain specific knowledge and heuristics to help the algorithm find the best move. In particular, I expected my AI to have difficulty ending the game. Chess has a fairly complicated win state, and it is computationally infeasible to simulate an entire game to completion on each iteration. From the get-go, I knew I would need to work around these constraints.

I will briefly cover the basics of MCTS before delving into the specifics of my implementation. Unlike many other search techniques which focus on depth, Monte Carlo Tree Search incrementally builds a search tree and estimates the success of moves by simulating the game from that point. It balances the exploration of new moves with the exploitation of known good moves, efficiently allocating resources primarily to the most promising parts of the search space. 
The main process can be broken down into four main components: Selection, Expansion, Simulation, and Backpropagation. In the Selection phase, the existing tree is traversed from the root to a leaf node, eventually selecting a node that either has a high win rate or that has not been explored much. In Expansion, so long as the chosen node does not represent a terminal state for the game, child nodes representing all possible future moves from the current state are created. Then, the Simulation occurs. This is where the potential of the newly added nodes is assessed by playing out the game to a predefined depth and seeing the effects of the different possible moves. Finally, the simulation results are Backpropagated up the tree, from the new leaf nodes to the root, based on the overall outcome. This typically means updating the visit count and win rate of each node. This is crucial, as it indicates to future iterations of the algorithm which branches of the tree are most promising and worth further exploration. The algorithm ultimately returns the best performing child node. 

I chose to complete this project in Python primarily due to the existence of libraries like python-chess, which handle the gameplay of chess and allow me to primarily focus on the AI itself. The first thing I set up was the chess_game.py file, defining the chess_game() function. I initially set this up to function as a command line game between two human players. Once I was happy with how it felt, I swapped out the second player for an AI. I then set up the skeleton of MCTS in a new file mcts.py and initially returned just a random move for each turn. I quickly realized that I would not be able to effectively test gameplay manually, so I set the AI to play against itself. The results were, as expected, not pretty. Games would go on for hundreds of turns, almost always ending in insufficient material draws. It was time to start working on the algorithm itself.

I set up a Node class to hold information about each state in the tree. I then filled in a dedicated method for each of the four components of MCTS, managed by the aptly named mcts() function. I replicated the core functionality of MCTS within these functions, and implemented a function evaluate_state() that was called after each run of the simulation and pushed the AI towards moves that would result in checkmate for itself. Somewhat shockingly, this worked really well in terms of reaching end states. Games would consistently end in a checkmate within a reasonable number of turns. However, the AI was not playing intelligently. In pushing towards checkmate, it was completely ignoring positioning, captures, and the safety of its own king. This current strategy would never work against a real opponent; it was clear that further improvements to the state evaluation were needed.

I started out by including checks and piece captures in the evaluation. Testing at this point was done against lower rated Chess.com bots. After seeing some initial success here, I decided on a lofty goal: I wanted to beat Stockfish on one of its lower settings. To this end, I continued to expand the state evaluation. This required a delicate balance, as too much being done in the state evaluation would make it infeasible to run enough simulations to get a good result, while more simulations but not enough evaluation of the current board would lead to poor move choices. I decided it was worthwhile to loop over the entire board once, checking and rewarding for key squares controlled by the player, piece safety, piece coordination, and threats to the opposing playerâ€™s pieces. The AI seemed to have difficulty realizing when one of its pieces was under imminent threat due to a recent move on the opposing side, so I revisited the expansion() function to include a heuristic that focuses on expanding nodes that move currently threatened pieces. Final improvements to the state evaluation include king safety, pawn structure, and piece mobility. Previously, the AI was too willing to move its king pointlessly or make equally useless 1 square moves of key pieces like rooks or the queen. These last changes helped to mitigate these issues.

So, what was the end result? Unfortunately, my AI was not able to secure a win against Stockfish, even on the lowest difficulty. However, the improvements described above have resulted in somewhat consistent draws against the titan of chess engines, so I was happy with the result. The AI lost against three human players but managed to draw one in a stalemate and beat another. It has been really exciting to watch my MCTS AI evolve from a glorified random move generator to something capable of intelligent play, coordinating attacks, choosing solid openings due to rewards from position control, and even winning games. However, I ultimately think that pure MCTS is not the most suitable for chess. Logical decisions require heavy state evaluation, which is likely to quickly consume available compute resources, in addition to needing to be hard coded. Manually defining everything required to perfect state evaluation would be heavily verbose to say the least, and would require massive game knowledge as well, far beyond my own. Combining MCTS with machine learning or taking a more ML focused approach from the get-go, would likely result in a much stronger engine much more quickly, in my opinion. Regardless, this project was extremely enjoyable to complete and definitely helped solidify my understanding of Monte Carlo Tree Search.
