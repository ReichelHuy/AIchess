{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL\n",
    "import glob\n",
    "from IPython.display import clear_output, Image, display\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "\n",
    "def display_array(a, rng=[0,255]):\n",
    "    a = (a - rng[0])/float(rng[1] - rng[0])*255\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    display(PIL.Image.fromarray(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a fen string and a rank (number) and file (letter), return label vector\n",
    "def get_label(fen,rank,file):\n",
    "    l2i = lambda l:  ord(l)-ord('A') # letter to index\n",
    "    file = 8-file # FEN has order backwards\n",
    "    piece_letter = fen[file*8 + file + l2i(rank)]\n",
    "    #label = np.zeros(13, dtype=np.uint8)\n",
    "    label = '1KQRBNPkqrbnp'.find(piece_letter)\n",
    "    # note the 1 instead of ' ' due to FEN notation\n",
    "    # We ignore shorter FENs with numbers > 1 because we generate the FENs ourselves\n",
    "    return label\n",
    "\n",
    "\n",
    "# Load Tiles with FEN string in filename for labels return both images and labels\n",
    "def load_tiles(image_filepaths):\n",
    "    # Each tile is a 32x32 grayscale image, add extra axis for working with MNIST Data format\n",
    "    images = np.zeros([image_filepaths.size, 32, 32], dtype=np.uint8)\n",
    "    labels = np.zeros([image_filepaths.size], dtype=np.uint8)\n",
    "\n",
    "    for i, image_filepath in enumerate(image_filepaths):\n",
    "        fen = image_filepath[-78:-7]\n",
    "        rank = image_filepath[-6]\n",
    "        file = int(image_filepath[-5]) \n",
    "\n",
    "        images[i,:,:] = np.asarray(PIL.Image.open(image_filepath), dtype=np.uint8)\n",
    "        labels[i] = get_label(fen, rank, file)\n",
    "    print(\"Done\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2649 Training tiles\n",
      "Done\n",
      "Loading 295 Testing tiles\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# All tiles with pieces in random organizations\n",
    "all_paths = np.array(glob.glob(\"train_tiles/*/*.png\"))\n",
    "\n",
    "# Shuffle order of paths so when we split the train/test sets the order of files doesn't affect it\n",
    "np.random.shuffle(all_paths)\n",
    "\n",
    "# split data into train/test\n",
    "\n",
    "ratio = 0.9 # split ratio\n",
    "divider = int(len(all_paths) * ratio)\n",
    "train_paths = all_paths[:divider]\n",
    "test_paths = all_paths[divider:]\n",
    "\n",
    "print(f\"Loading {train_paths.size} Training tiles\")\n",
    "train_images, train_labels = load_tiles(train_paths)\n",
    "\n",
    "print(f\"Loading {test_paths.size} Testing tiles\")\n",
    "test_images, test_labels = load_tiles(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682: Piece(b)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACDUlEQVR4nH2SPWhTYRSG3/PdL7m9N00w0RJNRK06mcW6FBGqS6GIQnBycNHBWWfBsa4ZSwenOBXRbLq0FAzUUUGpEGJpUqumCcaKN8m99xyHmJufpj3T9/NwznvOe6iEo0MPX4Wh6AiATRt/2+pQgK2tPO6dcwYJGtAgxp9sGedfT/oDVQZhjmyUw+HyRoQxHqDWdNR1YxccOgSQUPqyyEw6LOMBjr+Zf2cY6/Nv4zwO4GjuwQ58nyr3c1E+CPixV4ukaXaWNC0WYv4oIGb1KZHH2Sx7RE+qpowAbK/uac4UFhYKGdb1VZtHS/AaCWorlcpKDUJrgYjeqMndF8HP/BTygMi+SyOA6AiBSD2HZgHFQs5oCWNOIOw1mx4LZE71RPbMEuXe+XxSCUGIv196afYMC9xku/owN91S4Imvj5dTgeX9fVDFjze1ACCvVbwb2BEA5G/ekO78DPrCgaGBhlDt29n/vUlo6/Rxb1iDCLWvNYNyseKEEPXbZNaWlVqasQ0AMOwry2nL0r70MrBFtXrDM7fX3/9gqOTV6+mOTkwlHVcBVAJPbj778LstAEgBYAFAZvzWo5ijQCUJ79yuY1xcfJF0SQOQZDN1JiwAHUtk8KnxSwDqbO8aHgFUgpjObuKE7koSUPfg7TVOWW0ClQBRYc+VkfwU0h2m7iSJW0Pr3w3PA/VGTQd++4//AEQM3AP8evWcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x10F82F370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989: Piece(K)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAClElEQVR4nH2TS0iUURTH/+d+j5lxNLU08pEFojDma1EGBoqaEZhBIhVJEUiBtGjVSjdBiBFBE62iKLLAhQThwtJEUyGsSMbs4ZOsNGVEE2dyxu+732kxajMi3t2993fP4XJ+f3Jj+yU27Zm3B1hVeTuA1fn5TcSmCtG9vdGRJ2r4e/37zSV8Pb9/lbYGSMaf6UZJvAy7j2xhRRUlJhZFWVu1YIDAtgf90CuDxGCicIDJJizDiOl/+xKnEgv9Ng2GAdoALAdPLTlT49DZWIzGG+WYm7ZSk4OGWAMsx3gLErz+nOOaSyJLdHUtJ+NX0oW4VQIEYNkn3Vc8H4aad92anSdl4WdP3fvBwYG8pkWNAXKDHQ111wEAbRfLW4zDKU/iAQC1nfUBggq2j6jXJBEsq/JV4es+3wthCoJFd9PHMgKkgvXPudFSAIpiHHxUpXcLqQJQ2Jnxw8UkACtuIfQjaGaN61COVACA2ZzbYQECFMj3vBMm2DJNyyrJlaYpGTBFy0xegCBARkLJ2SXVIKGqugApNlUhaWifLp22SwK5AXbck60ueNpHpxa/KAW7M4qPROFNVUH1skAIIPvj8eo+b05eVpJTzo54hoLntDsnji0LhAAwJzz82HwSf6Z83lhn2h4M1g427POJjVkQB6KL7RXD/h2OGH9wQd97+ShletXwcbP+fLKm6UDIjt+dT3uy/UqED8pKaddY++1JX1CJSc4v6qhv1QMUUUEdLZ1Jza5KVLA4PfAsynLICKPITGlbuV+xrtm3qx1l62KSe62Ct2d4pys9RWPv2ORESln6ejrWALCmL45OLPhM4YxNy0xCEJsAMKs2jSWTEDJo8IbtG7kgkn8ZFPKb/icjLDgUGnp4agD8Aw4bCQlfnUGGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x141629BB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: Piece(r)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABtUlEQVR4nIVTPU9UQRQ9977ZN6xETZYNnY2EUIAEzdb0NkQbf4AdsfdHWGlF5V8ATCyoCQUhFFvYICx0ZqME4sqy782bmWvBvNm3u3zcZm4yZ845c+4MfcH9pe7eIkDuBVgIcwRIPBIW8o15czRgLqW0TgFKtU4JSLVWbulq6rmlwMB/r5A02HYFNJP4Pw6P6yfL2Q8lNwBJtw5BH1d2NwCsr7Y/CVrvm7/qM70kMIhT7DZ3uwlBdtrnnDgrs6d6tlOapNwCx8cAgLMzwMHw4bz5WQsSlL18nYAhAoAIHq5rBvuoUbzmUqdvZJhROr2IRFWDenJ59KagMpTa9sJTC6lGbdY2v1NJQTK3dsFjs8h/tysx1/OyiwBSzD707BVNAAAPP2xjMR6oBwFDCUq4FGaiSQAZ58rewUyaNM8W05iD6ZkxgOjL3nIl6r2LRiAJJrn/reVMEcq41nZ/JElR5uBg1P3bRzejCeO22SslFQmymaXIILXzjf67uXyYidedz9PrzYICA1938RVj9e+ao0m59fuo+B6oaH7IWEZ2yU81C4om+QWNn4fklWticIsEVwF3D/U/1KGmBsWvvZkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x141629A90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420: Piece(Q)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAADJUlEQVR4nH2TS2yUVRiG3+/8l5nOP+P071ys05KWFohYOyMhgsYFIUITLws3qAQyaiJqTKQsidqkiYGyaKIJLOwChdgaxIWJVyQhIREWSJUiAWaaUVLrONPL3869/W/nuBg6DsT47b7kOe93kvd9KYP/H/neVXAw+m+AM3AmlADKNjVjbO2pxmQfV41DhwyVS6GwLJoA7gruu7D72ckWDC4sDpJsHhkqKUK4HABlwAOeIrd2Dc1NXKhuuW4//ot+IBdwxk0laJYZZAjtfOaF9poVI8sKvvG0eF0v/PRj7GGjPffJhl01wnR+NJxI3Mif8LZMZNPG5keMdO7g+vX7cjcSifBofhqpQnxY+Mez5VdCdzKZdGdnOvP7TOvOXP6zoBiOF1KMzD1jA53xWrXF+MYvZw0jq3iuFvuUUiI2MLbHJCaVk0fmaxIYV761vVN9fVMqOytsl7HK/NFkWWLAyt7Df44EUNo9O9l6dfv2yQduTz1RJe3o7OGXVgAGsAWffuacXn7sqc+l21u33lLP9u8o6Oe+0H2LDGAA8VhH8v2K7Lx98Y+5eDw/+/U7JFWGXu2IcaoDTnRlb+9wZHmL/lGwqzt43LNjKTzc+/JK1KkDcLV1106evuThyY+jobYHj+/n6uXTJ6+t09w1s+SeK73HUmH2lqefqF86yEKpY71XeuSG3VbfqTM5fPhVdXXiIrLmNu9fuPNp+jWL6maBa789s8kb725XHiIVlsjbuZnrtcwP8SqrK7DVjV3J9+6L2genNq3S2gnXf+JFj3mzWLYAAGog+Kg68qW3yu6eAJzI2LvPbWyL+IQEl2oLS9Pfj7w5L6MBgPufH99sLCxZVWhqWyR0a/93FdYUWu5N3dzJJI8CH2qwTZcXUxtW2b8KQhvoHgx1tKoAAKuQXRwVE8tSQ4H7L+V/bgGEIwSIlGgUOFCsB5sBgPBeDswvOiBZUVVFJthGoJTxiIYCK+37dZvSpqtBVYFtFc2lov1kj0lNf1DF3PLfM9WKycE8fq0rpkfIaQIgoMqSQowESHBhu44tqLmbBMuEgLi7EIjovvLSvaVuzD9ZqFgMZ37vtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x141629BE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692: Piece(K)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACoUlEQVR4nH2TX0jTURTHz733t1/T+adJs0wzwT+lS7MwEQrFkHpIERwoZC8paVAPPlRQRPhoD70JvUgpJEII2UMIiViBhVialTHdr3TZliabLjf27957epizucrzdM+9X76Xc87nEA12DhqXI+4sQFXFnQSo2u1xijgHY1+f8f8OUm8tGxwss+plzCWJqQIVt/YAWvLSOPm3A+EpjTk5jSmx76BE7QEIgYRrA5jc4QdEICRWIEkik8FA2uP7z6D2oMVjUCEUiEiIBgDSgLOru/P3QNXlK9Dd82Z90cYPFfiDdNNBJk12uowucqaNFgow897eBROslHZl+BgABZCGqSbL9LuZ7uX6BRdhntm7ta/fvx9PrXMkIABoms15+BYiIuJTYxOGjtWuRbILx502TQNt3vkkY4MLKXkIJ3Ujd/IFhoWUXHjTh5zzmgKoHzcnCQrAWLjsYYM6RoUCAAwNedZTCBRAmNwQKVrHmwtPFAsGAIDIV9IEAQrUVzM3QTmg5FzK6hLBuUAATvs3qr0EKJBgdpvllxImVFFUCoTtUhgRYd3HS50pnADRADCp1TZUBDPD8/a1z6w8Pb/qZCK8qG+57WKRTiIxXB+xvFotPlqUYRDLczMfgs20+2a7i0VbjZjV0T9QB+t272qqIXsfTF/8MlSyTreGRYTPVKk/98mXkpDsC7rVA+2ncyu+qTHTJEiHl853mSN0/Bh5NHbWo2zjgQUqX9qG7331Blny/tLK5zemVO82ATJ7jSPzSIOJwZpjot8QzoxyuflFKH/J31MXxcx6dbRckG3QorrQO6oW5Gbp8Kdt8XtJa0XUIUo16vXOt1MOd4ga95rLCsD3F/YS1URVCkmYEvYHkMVTDRS4B4EAIBIaswzKnyOJXYet+A2j+iu/vHUX1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x10F82F370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert label index into name of piece\n",
    "def labelIndex2Name(label_index):\n",
    "    return ' KQRBNPkqrbnp'[label_index]\n",
    "\n",
    "\n",
    "# Convert label vector into name of piece\n",
    "def label2Name(label):\n",
    "    return labelIndex2Name(label.argmax())\n",
    "\n",
    "\n",
    "# check some of the labels\n",
    "for i in np.random.choice(len(train_images), 5, replace=False):\n",
    "    print(f\"{i}: Piece({labelIndex2Name(train_labels[i])})\")\n",
    "    display_array(train_images[i,:,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86 0.86 0.86 ... 0.86 0.86 0.86]\n",
      " [0.86 0.86 0.86 ... 0.86 0.86 0.86]\n",
      " [0.86 0.86 0.86 ... 0.86 0.86 0.86]\n",
      " ...\n",
      " [0.86 0.86 0.86 ... 0.86 0.86 0.86]\n",
      " [0.86 0.86 0.86 ... 0.86 0.86 0.86]\n",
      " [0.86 0.86 0.86 ... 0.86 0.86 0.86]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize Images\n",
    "\n",
    "train_norm_images = np.zeros([len(train_images), 32, 32], dtype=np.float64)\n",
    "test_norm_images = np.zeros([len(test_images), 32, 32], dtype=np.float64)\n",
    "\n",
    "for i in range(len(train_images)):\n",
    "    train_norm_images[i,:,:] = train_images[i,:,:] / 255.0\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    test_norm_images[i,:,:] = test_images[i,:,:] / 255.0\n",
    "    \n",
    "print(train_norm_images[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 132,877\n",
      "Trainable params: 132,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build our keras neural net\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(32, 32)),  \n",
    "                                    tf.keras.layers.Dense(128, activation = tf.nn.relu),  \n",
    "                                    tf.keras.layers.Dense(13, activation = tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics =['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1925 - accuracy: 0.5157\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8557 - accuracy: 0.8494\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8377 - accuracy: 0.8494\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8039 - accuracy: 0.8852\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7647 - accuracy: 0.9226\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7374 - accuracy: 0.9536\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6900 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6896 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6895 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6894 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1417bb730>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_norm_images, train_labels, epochs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1.6894 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_norm_images,  test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/softmax_v1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/softmax_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
